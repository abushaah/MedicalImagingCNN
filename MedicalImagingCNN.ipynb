{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:07.640301Z","iopub.execute_input":"2025-07-28T23:41:07.640726Z","iopub.status.idle":"2025-07-28T23:41:10.838926Z","shell.execute_reply.started":"2025-07-28T23:41:07.640700Z","shell.execute_reply":"2025-07-28T23:41:10.838224Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: monai in /usr/local/lib/python3.11/dist-packages (1.5.0)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom glob import glob\nimport psutil\n\n\"\"\"\n    Get data inputs, assumes CT images and label masks have corresponding names and indices.\n    Analyze the nifti datasets for MONAI parameter adjustments\n    :param str in_dir: file path of data.\n\"\"\"\ndef prepare_and_configure(in_dir):\n    image_dict = {}\n    label_dict = {}\n\n    # find all .nii files under in_dir\n    nii_files = glob(os.path.join(in_dir, \"**\", \"*.nii\"), recursive=True)\n\n    for filepath in nii_files:\n        filename = os.path.basename(filepath)\n        if filename.startswith(\"volume-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            image_dict[idx] = filepath\n        elif filename.startswith(\"segmentation-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            label_dict[idx] = filepath\n\n    # match image and label by idx\n    \n    # matched_keys = sorted(set(image_dict.keys()) & set(label_dict.keys()))\n    # test model syntax first\n    matched_keys = sorted(set(image_dict.keys()) & set(label_dict.keys()))[:20]\n\n    all_files = [{\"image\": image_dict[k], \"label\": label_dict[k]} for k in matched_keys]\n\n    # split 80% train / 20% validation\n    split_idx = int(0.8 * len(all_files))\n    train_files = all_files[:split_idx]\n    validation_files = all_files[split_idx:]\n    \n    # analyze voxel sizes and shapes\n    voxel_sizes = []\n    shapes = []\n    for k in matched_keys:\n        img = nib.load(image_dict[k])\n        data = img.get_fdata()\n        voxel_sizes.append(img.header.get_zooms())\n        shapes.append(data.shape)\n\n    # pixdim based on variables in https://github.com/Project-MONAI/tutorials/blob/main/3d_label/spleen_label_3d.ipynb\n    mean_spacing = np.mean(voxel_sizes, axis=0)\n    mean_shape = np.mean(shapes, axis=0)\n\n    if isinstance(mean_spacing, np.ndarray):  # Expected case\n        pixdim = tuple(round(s, 2) for s in mean_spacing)\n    else:  # Fallback for scalar\n        pixdim = (round(mean_spacing, 2),)        \n\n    # default for soft tissue\n    a_min, a_max = -200, 250\n\n    # detect GPU & RAM memory\n    try:\n        import GPUtil\n        gpus = GPUtil.getGPUs()\n        mem_free_gpu = max([gpu.memoryFree for gpu in gpus])  # in MB\n    except Exception:\n        mem_free_gpu = 0  # fallback to CPU\n\n    mem_free_ram = psutil.virtual_memory().available // (1024 * 1024)\n\n    # adjust preprocessing resolution based on memory\n    # values are randomized based on https://docs.monai.io/en/stable/transforms.html\n    if mem_free_gpu >= 20000:\n        spatial_size = [256, 256, 256]\n        batch_size = 2\n    elif mem_free_gpu >= 10000:\n        spatial_size = [192, 192, 128]\n        batch_size = 1\n    elif mem_free_gpu >= 4000:\n        spatial_size = [128, 128, 64]\n        batch_size = 1\n    else:\n        spatial_size = [96, 96, 64]\n        batch_size = 1\n\n    return {\n        \"train_files\": train_files,\n        \"validation_files\": validation_files,\n        \"pixdim\": pixdim,\n        \"a_min\": a_min,\n        \"a_max\": a_max,\n        \"spatial_size\": spatial_size,\n        \"batch_size\": batch_size,\n        \"mem_free_gpu\": mem_free_gpu,\n        \"mem_free_ram\": mem_free_ram,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:10.839915Z","iopub.execute_input":"2025-07-28T23:41:10.840293Z","iopub.status.idle":"2025-07-28T23:41:11.101764Z","shell.execute_reply.started":"2025-07-28T23:41:10.840259Z","shell.execute_reply":"2025-07-28T23:41:11.101207Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import re\nfrom glob import glob\nfrom monai.transforms import (\n    Compose,\n    EnsureChannelFirstD,\n    LoadImaged,\n    Resized,\n    ToTensord,\n    Spacingd,\n    Orientationd,\n    ScaleIntensityRanged,\n    CropForegroundd,\n    RandCropByPosNegLabeld,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism\n\n\"\"\"\n    Use MONAI transforms to prepares data for segmentation.\n    Voxel: 3D grid representation of data.\n    \n    :param tuple pixdim: standard voxel spacing (in millimeters) for resampling the images in the x, y, and z dimensions.\n    :param int a_min: intensity voxel min for CT scans (less are clipped before scaling).\n    :param int a_max: intensity voxel max for CT scans (more are clipped before scaling).\n    :param int array spatial_size: output size (in voxel) to which each image and label volume will be resized. AKA input size for the neural network.\n    :param int batch_size: adjyst batch size, default is 1.\n    :return PyTorch DataLoader objects: used to train neural network.\n\"\"\"\ndef preprocess(pixdim, a_min, a_max, spatial_size, batch_size, cache, train_files, validation_files):\n\n    # reproduce training results\n    set_determinism(seed=0)\n\n    # and apply transformations to them\n    # parameters from https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n    train_transforms = Compose([\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstD(keys=[\"image\", \"label\"]),\n        ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"image\", \"label\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        RandCropByPosNegLabeld(\n            keys=[\"image\", \"label\"],\n            label_key=\"label\",\n            spatial_size=spatial_size,  # use your configured size here\n            pos=1, neg=1,\n            num_samples=4,\n            image_key=\"image\",\n            image_threshold=0,\n        ),\n        ToTensord(keys=[\"image\", \"label\"]),\n    ])\n\n    # transforms for validation data\n    validation_transforms = Compose([\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstD(keys=[\"image\", \"label\"]),\n        ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"image\", \"label\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n    ])\n\n    if cache is not None and cache >= 16000:\n        train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0)\n        validation_ds = CacheDataset(data=validation_files, transform=validation_transforms, cache_rate=1.0)\n\n        # train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n        # validation_ds = CacheDataset(data=validation_files, transform=validation_transforms, cache_rate=1.0, num_workers=4)\n    else:\n        train_ds = Dataset(data=train_files, transform=train_transforms)\n        validation_ds = Dataset(data=validation_files, transform=validation_transforms)\n\n        # train_ds = Dataset(data=train_files, transform=train_transforms, num_workers=4)\n        # validation_ds = Dataset(data=validation_files, transform=validation_transforms, num_workers=4)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size)\n    validation_loader = DataLoader(validation_ds, batch_size=batch_size)\n\n    # use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    # train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n    # validation_loader = DataLoader(validation_ds, batch_size=batch_size, num_workers=4)\n\n    return train_loader, validation_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:11.102447Z","iopub.execute_input":"2025-07-28T23:41:11.102781Z","iopub.status.idle":"2025-07-28T23:41:21.299734Z","shell.execute_reply.started":"2025-07-28T23:41:11.102766Z","shell.execute_reply":"2025-07-28T23:41:21.299145Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-07-28 23:41:17.984505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753746078.007105     289 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753746078.013974     289 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from monai.transforms import (\n    AsDiscrete,\n    Compose,\n)\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef train (model, train_loader, validation_loader, loss_function, optimizer, dice_metric):\n    max_epochs = 600\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = []\n    metric_values = []\n    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n    post_label = Compose([AsDiscrete(to_onehot=2)])\n    scaler = GradScaler()\n    \n    for epoch in range(max_epochs):\n        print(\"-\" * 10)\n        print(f\"epoch {epoch + 1}/{max_epochs}\")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        print(f\"Number of batches in train_loader: {len(train_loader)}\")\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = (\n                batch_data[\"image\"].to(device), # volume\n                batch_data[\"label\"].to(device), # segment\n            )\n\n            # labels.shape: torch.Size([4, 1, 96, 96, 64]), dtype: torch.float32\n            # unique label values: tensor([0., 1., 2.], device='cuda:0')\n            labels = labels.squeeze(1) # remove channel dimension [B, D, H, W]\n            labels = labels.long()\n            labels = torch.clamp(labels, 0, 1)     # ensure binary labels\n            \n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(inputs)\n\n                print(f\"outputs.shape: {outputs.shape}, dtype: {outputs.dtype}\")\n                print(f\"labels.shape: {labels.shape}, dtype: {labels.dtype}\")\n                print(f\"unique label values: {torch.unique(labels)}\")\n                \n                loss = loss_function(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n                \n            # loss.backward()\n            # optimizer.step()\n            print(f\"loss.item: {loss.item()}\")\n            epoch_loss += loss.item()\n            print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n    \n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                for validation_data in validation_loader:\n                    val_inputs, val_labels = (\n                        validation_data[\"image\"].to(device),\n                        validation_data[\"label\"].to(device),\n                    )\n                    # roi_size = (160, 160, 160)\n                    roi_size = (64, 64, 64)\n                    sw_batch_size = 4\n                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n                    # compute metric for current iteration\n                    dice_metric(y_pred=val_outputs, y=val_labels)\n\n                    # free memory\n                    del val_inputs, val_labels, val_outputs\n                    torch.cuda.empty_cache()\n    \n                # aggregate the final mean dice result\n                metric = dice_metric.aggregate().item()\n                # reset the status for next validation round\n                dice_metric.reset()\n    \n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n                    print(\"saved new best metric model\")\n                print(\n                    f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n                    f\"\\nbest mean dice: {best_metric:.4f} \"\n                    f\"at epoch: {best_metric_epoch}\"\n                )\n                \n    print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n    plot(epoch_loss_values, val_interval, metric_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:21.301716Z","iopub.execute_input":"2025-07-28T23:41:21.302291Z","iopub.status.idle":"2025-07-28T23:41:21.313675Z","shell.execute_reply.started":"2025-07-28T23:41:21.302271Z","shell.execute_reply":"2025-07-28T23:41:21.312971Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot(epoch_loss_values, val_interval, metric_values):\n    plt.figure(\"train\", (12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Epoch Average Loss\")\n    x = [i + 1 for i in range(len(epoch_loss_values))]\n    y = epoch_loss_values\n    plt.xlabel(\"epoch\")\n    plt.plot(x, y)\n    plt.subplot(1, 2, 2)\n    plt.title(\"Val Mean Dice\")\n    x = [val_interval * (i + 1) for i in range(len(metric_values))]\n    y = metric_values\n    plt.xlabel(\"epoch\")\n    plt.plot(x, y)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:21.314382Z","iopub.execute_input":"2025-07-28T23:41:21.314778Z","iopub.status.idle":"2025-07-28T23:41:21.330246Z","shell.execute_reply.started":"2025-07-28T23:41:21.314760Z","shell.execute_reply":"2025-07-28T23:41:21.329672Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\nfrom monai.metrics import DiceMetric\nfrom monai.networks.layers import Norm\n\n# if __name__ == '__main__':\n    \n# 1. user input (for now, it is kaggle data set)\nprint(\"Preparing dataset directory and machine specifications...\")\nparams = prepare_and_configure(in_dir=\"/kaggle/input\")\nprint(\"Complete\")\n\n# show output\n# print(\"prepare_and_configure:\")\n# for k, v in params.items():\n#     print(f\"{k}: {v}\")\n\n# 2. preprocess\nprint(\"Preprocess dataset with MONAI transforms...\")\ntrain_loader, validation_loader = preprocess(pixdim=params['pixdim'], a_min=params['a_min'], a_max=params['a_max'], spatial_size=params['spatial_size'], batch_size=params['batch_size'], cache=params['mem_free_ram'], train_files=params['train_files'], validation_files=params['validation_files'])\n# print(train_loader)\n# print(validation_loader)\nprint(\"Complete\")\n\n# 3. build U-net\nprint(\"Building U-Net Model...\")\ndevice = torch.device(\"cuda:0\")\nmodel = UNet(\n    spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(8, 16, 32, 64),\n    strides=(2, 2, 2, 2),\n    num_res_units=1,\n    norm=Norm.BATCH,\n).to(device)\nprint(\"Complete\")\n\nprint(\"Evaluating model parameters...\")\nloss_function = DiceLoss(to_onehot_y=True, softmax=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-4)\ndice_metric = DiceMetric(include_background=False, reduction=\"mean\")\nprint(\"Complete\")\n\n# 4. train\nprint(\"Training model...\")\ntrain(model, train_loader, validation_loader, loss_function, optimizer, dice_metric)\nprint(\"Complete\")\n\n# 5. test/validate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:41:21.331075Z","iopub.execute_input":"2025-07-28T23:41:21.331341Z","iopub.status.idle":"2025-07-28T23:47:58.785385Z","shell.execute_reply.started":"2025-07-28T23:41:21.331316Z","shell.execute_reply":"2025-07-28T23:47:58.783627Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset directory and machine specifications...\nComplete\nPreprocess dataset with MONAI transforms...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 16/16 [04:58<00:00, 18.63s/it]\nLoading dataset: 100%|██████████| 4/4 [01:31<00:00, 22.76s/it]\n/usr/local/lib/python3.11/dist-packages/monai/networks/nets/unet.py:130: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n","output_type":"stream"},{"name":"stdout","text":"Complete\nBuilding U-Net Model...\nComplete\nEvaluating model parameters...\nComplete\nTraining model...\n----------\nepoch 1/600\nNumber of batches in train_loader: 16\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_289/2161017384.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_289/2161017384.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"outputs.shape: torch.Size([4, 2, 96, 96, 64]), dtype: torch.float16\nlabels.shape: torch.Size([4, 96, 96, 64]), dtype: torch.int64\nunique label values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_289/3027417431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# 4. train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_289/2161017384.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, validation_loader, loss_function, optimizer, dice_metric)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"unique label values: {torch.unique(labels)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/losses/dice.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single channel prediction, `to_onehot_y=True` ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_pred_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_background\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/networks/utils.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(labels, num_classes, dtype, dim)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels should have a channel with length equal to one.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: labels should have a channel with length equal to one."],"ename":"AssertionError","evalue":"labels should have a channel with length equal to one.","output_type":"error"}],"execution_count":6}]}