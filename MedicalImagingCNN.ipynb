{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n    Analyze the nifti datasets for MONAI parameter adjustments\n\"\"\"\nimport os\nimport nibabel as nib\nimport numpy as np\nfrom glob import glob\n\ndef analyze_nii_dataset(volume_paths):\n    # analyze voxel size, shape, and intensity based on input data\n    voxel_sizes = []\n    shapes = []\n    for path in volume_paths:\n        img = nib.load(path)\n        data = img.get_fdata()\n        voxel_sizes.append(img.header.get_zooms())\n        shapes.append(data.shape)\n\n    mean_spacing = np.mean(voxel_sizes, axis=0)\n    mean_shape = np.mean(shapes, axis=0)\n\n    pixdim = tuple(round(s, 2) for s in mean_spacing)\n    a_min, a_max = -200, 250\n\n    # estimate spatial_size and batch_size based on GPU memory\n    try:\n        import GPUtil\n        gpus = GPUtil.getGPUs()\n        mem_free = max([gpu.memoryFree for gpu in gpus])  # in MB\n    except Exception:\n        mem_free = 0  # CPU\n\n    # configure based on memory (very rough estimates)\n    if mem_free >= 20000:\n        spatial_size = [256, 256, 256]\n        batch_size = 2\n    elif mem_free >= 10000:\n        spatial_size = [192, 192, 128]\n        batch_size = 1\n    elif mem_free >= 4000:\n        spatial_size = [128, 128, 64]\n        batch_size = 1\n    else:\n        spatial_size = [96, 96, 64]\n        batch_size = 1  # fallback for low-mem or CPU\n\n    return {\n        'pixdim': pixdim,\n        'a_min': a_min,\n        'a_max': a_max,\n        'spatial_size': spatial_size,\n        'batch_size': batch_size,\n        'mem_free_MB': mem_free\n    }\n\nvolume_paths = glob(\"/kaggle/input/**/volume-*.nii\", recursive=True)\nparams = auto_configure_parameters(volume_paths)\n\n# print(\"analyze_nii_dataset:\")\n# for k, v in params.items():\n#     print(f\"{k}: {v}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:30:56.738865Z","iopub.execute_input":"2025-06-19T20:30:56.739274Z","iopub.status.idle":"2025-06-19T20:38:02.715208Z","shell.execute_reply.started":"2025-06-19T20:30:56.739239Z","shell.execute_reply":"2025-06-19T20:38:02.713515Z"}},"outputs":[{"name":"stdout","text":"Auto-configured parameters:\npixdim: (0.79, 0.79, 1.51)\na_min: -200\na_max: 250\nspatial_size: [96, 96, 64]\nbatch_size: 1\nmem_free_MB: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom glob import glob\n\n\"\"\"\n    Get data inputs, assumes CT volumes and segmentation masks have corresponding names and indices.\n    :param str in_dir: file path of data.\n\"\"\"\ndef prepare(in_dir=\"/kaggle/input\"):\n\n    volume_dict = {}\n    segmentation_dict = {}\n    \n    # find all .nii files under in_dir\n    nii_files = glob(os.path.join(in_dir, \"**\", \"*.nii\"), recursive=True)\n    \n    for filepath in nii_files:\n        filename = os.path.basename(filepath)\n        if filename.startswith(\"volume-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            volume_dict[idx] = filepath\n        elif filename.startswith(\"segmentation-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            segmentation_dict[idx] = filepath\n\n    # match volume and segmentation by idx\n    matched_keys = sorted(set(volume_dict.keys()) & set(segmentation_dict.keys()))\n    all_files = [{\"vol\": volume_dict[k], \"seg\": segmentation_dict[k]} for k in matched_keys]\n\n    # split into 80 train and 20 test\n    split_idx = int(0.8 * len(all_files))\n    train_files = all_files[:split_idx]\n    test_files = all_files[split_idx:]\n\n    return train_files, test_files","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nWritten by Haifaa with sources:\nhttps://github.com/Project-MONAI/tutorials\n\"\"\"\n\nimport re\nfrom glob import glob\nfrom monai.transforms import (\n    Compose,\n    EnsureChannelFirstD,\n    LoadImaged,\n    Resized,\n    ToTensord,\n    Spacingd,\n    Orientationd,\n    ScaleIntensityRanged,\n    CropForegroundd,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism\n\n\"\"\"\n    Use MONAI transforms to prepares data for segmentation.\n    Voxel: 3D grid representation of data.\n    \n    :param tuple pixdim: standard voxel spacing (in millimeters) for resampling the images in the x, y, and z dimensions.\n    :param int a_min: intensity voxel min for CT scans (less are clipped before scaling).\n    :param int a_max: intensity voxel max for CT scans (more are clipped before scaling).\n    :param int array spatial_size: output size (in voxel) to which each image and label volume will be resized. AKA input size for the neural network.\n    :param int batch_size: adjyst batch size, default is 1.\n    :return PyTorch DataLoader objects: used to train neural network.\n\"\"\"\ndef preprocess(pixdim=params['pixdim'], a_min=params['a_min'], a_max=params['a_max'], spatial_size=params['spatial_size'], batch_size=params['batch_size']):\n    train_files, test_files = prepare()\n\n    # reproduce training results\n    set_determinism(seed=0)\n\n    # and apply transformations to them\n    train_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n\n    # transforms for test data\n    test_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n    \n    train_ds = Dataset(data=train_files, transform=train_transforms)\n    test_ds = Dataset(data=test_files, transform=test_transforms)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size)\n    test_loader = DataLoader(test_ds, batch_size=batch_size)\n\n    return train_loader, test_loader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}