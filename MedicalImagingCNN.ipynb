{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-20T21:25:28.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom glob import glob\nimport psutil\n\n\"\"\"\n    Get data inputs, assumes CT volumes and segmentation masks have corresponding names and indices.\n    Analyze the nifti datasets for MONAI parameter adjustments\n    :param str in_dir: file path of data.\n\"\"\"\ndef prepare_and_configure(in_dir):\n    volume_dict = {}\n    segmentation_dict = {}\n\n    # find all .nii files under in_dir\n    nii_files = glob(os.path.join(in_dir, \"**\", \"*.nii\"), recursive=True)\n\n    for filepath in nii_files:\n        filename = os.path.basename(filepath)\n        if filename.startswith(\"volume-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            volume_dict[idx] = filepath\n        elif filename.startswith(\"segmentation-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            segmentation_dict[idx] = filepath\n\n    # match volume and segmentation by idx\n    matched_keys = sorted(set(volume_dict.keys()) & set(segmentation_dict.keys()))\n    all_files = [{\"vol\": volume_dict[k], \"seg\": segmentation_dict[k]} for k in matched_keys]\n\n    # split 80% train / 20% validation\n    split_idx = int(0.8 * len(all_files))\n    train_files = all_files[:split_idx]\n    validation_files = all_files[split_idx:]\n    \n    # analyze voxel sizes and shapes\n    voxel_sizes = []\n    shapes = []\n    for k in matched_keys:\n        img = nib.load(volume_dict[k])\n        data = img.get_fdata()\n        voxel_sizes.append(img.header.get_zooms())\n        shapes.append(data.shape)\n\n    # pixdim based on variables in https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n    mean_spacing = np.mean(voxel_sizes, axis=0)\n    mean_shape = np.mean(shapes, axis=0)\n    pixdim = tuple(round(s, 2) for s in mean_spacing)\n\n    # default for soft tissue\n    a_min, a_max = -200, 250\n\n    # detect GPU & RAM memory\n    try:\n        import GPUtil\n        gpus = GPUtil.getGPUs()\n        mem_free_gpu = max([gpu.memoryFree for gpu in gpus])  # in MB\n    except Exception:\n        mem_free_gpu = 0  # fallback to CPU\n\n    mem_free_ram = psutil.virtual_memory().available // (1024 * 1024)\n\n    # adjust preprocessing resolution based on memory\n    # values are randomized based on https://docs.monai.io/en/stable/transforms.html\n    if mem_free_gpu >= 20000:\n        spatial_size = [256, 256, 256]\n        batch_size = 2\n    elif mem_free_gpu >= 10000:\n        spatial_size = [192, 192, 128]\n        batch_size = 1\n    elif mem_free_gpu >= 4000:\n        spatial_size = [128, 128, 64]\n        batch_size = 1\n    else:\n        spatial_size = [96, 96, 64]\n        batch_size = 1\n\n    return {\n        \"train_files\": train_files,\n        \"validation_files\": validation_files,\n        \"pixdim\": pixdim,\n        \"a_min\": a_min,\n        \"a_max\": a_max,\n        \"spatial_size\": spatial_size,\n        \"batch_size\": batch_size,\n        \"mem_free_gpu\": mem_free_gpu,\n        \"mem_free_ram\": mem_free_ram,\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-20T21:25:28.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom glob import glob\nfrom monai.transforms import (\n    Compose,\n    EnsureChannelFirstD,\n    LoadImaged,\n    Resized,\n    ToTensord,\n    Spacingd,\n    Orientationd,\n    ScaleIntensityRanged,\n    CropForegroundd,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism\n\n\"\"\"\n    Use MONAI transforms to prepares data for segmentation.\n    Voxel: 3D grid representation of data.\n    \n    :param tuple pixdim: standard voxel spacing (in millimeters) for resampling the images in the x, y, and z dimensions.\n    :param int a_min: intensity voxel min for CT scans (less are clipped before scaling).\n    :param int a_max: intensity voxel max for CT scans (more are clipped before scaling).\n    :param int array spatial_size: output size (in voxel) to which each image and label volume will be resized. AKA input size for the neural network.\n    :param int batch_size: adjyst batch size, default is 1.\n    :return PyTorch DataLoader objects: used to train neural network.\n\"\"\"\ndef preprocess(pixdim, a_min, a_max, spatial_size, batch_size, cache, train_files, validation_files):\n\n    # reproduce training results\n    set_determinism(seed=0)\n\n    # and apply transformations to them\n    # parameters from https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n    train_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        RandCropByPosNegLabeld(\n            keys=[\"vol\", \"seg\"],\n            label_key=\"seg\",\n            spatial_size=spatial_size,  # use your configured size here\n            pos=1, neg=1,\n            num_samples=4,\n            image_key=\"vol\",\n            image_threshold=0,\n        ),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n\n    # transforms for validation data\n    validation_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n    ])\n\n    if cache >= 16000:\n        train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0)\n        val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0)\n\n        # train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n        # val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n    else:\n        train_ds = Dataset(data=train_files, transform=train_transforms)\n        validation_ds = Dataset(data=validation_files, transform=validation_transforms)\n\n        # train_ds = Dataset(data=train_files, transform=train_transforms, num_workers=4)\n        # validation_ds = Dataset(data=validation_files, transform=validation_transforms, num_workers=4)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size)\n    validation_loader = DataLoader(validation_ds, batch_size=batch_size)\n\n    # use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    # train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n    # validation_loader = DataLoader(validation_ds, batch_size=batch_size, num_workers=4)\n\n    return train_loader, validation_loader","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-20T21:25:28.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# usage flow\n\n# 1. user input (for now, it is kaggle data set)\nparams = prepare_and_configure(in_dir=\"/kaggle/input\")\n\n# testing\nprint(\"prepare_and_configure:\")\nfor k, v in params.items():\n    print(f\"{k}: {v}\")\n\n# preprocess & show reasoning\ntrain_loader, validation_loader = preprocess(pixdim=params['pixdim'], a_min=params['a_min'], a_max=params['a_max'], spatial_size=params['spatial_size'], batch_size=params['batch_size'], cache=params['mem_free_ram'], train_files=params['train_files'], validation_files=params['validation_files'])\nprint(train_loader)\nprint(validation_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-20T21:25:28.553Z"}},"outputs":[],"execution_count":null}]}