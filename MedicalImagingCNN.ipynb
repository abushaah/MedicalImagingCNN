{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T00:19:33.368004Z","execution_failed":"2025-07-05T04:37:44.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom glob import glob\nimport psutil\n\n\"\"\"\n    Get data inputs, assumes CT volumes and segmentation masks have corresponding names and indices.\n    Analyze the nifti datasets for MONAI parameter adjustments\n    :param str in_dir: file path of data.\n\"\"\"\ndef prepare_and_configure(in_dir):\n    volume_dict = {}\n    segmentation_dict = {}\n\n    # find all .nii files under in_dir\n    nii_files = glob(os.path.join(in_dir, \"**\", \"*.nii\"), recursive=True)\n\n    for filepath in nii_files:\n        filename = os.path.basename(filepath)\n        if filename.startswith(\"volume-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            volume_dict[idx] = filepath\n        elif filename.startswith(\"segmentation-\"):\n            idx = int(filename.split(\"-\")[1].split(\".\")[0])\n            segmentation_dict[idx] = filepath\n\n    # match volume and segmentation by idx\n    \n    # matched_keys = sorted(set(volume_dict.keys()) & set(segmentation_dict.keys()))\n    # test model syntax first\n    matched_keys = sorted(set(volume_dict.keys()) & set(segmentation_dict.keys()))[:10]\n\n    all_files = [{\"vol\": volume_dict[k], \"seg\": segmentation_dict[k]} for k in matched_keys]\n\n    # split 80% train / 20% validation\n    split_idx = int(0.8 * len(all_files))\n    train_files = all_files[:split_idx]\n    validation_files = all_files[split_idx:]\n    \n    # analyze voxel sizes and shapes\n    voxel_sizes = []\n    shapes = []\n    for k in matched_keys:\n        img = nib.load(volume_dict[k])\n        data = img.get_fdata()\n        voxel_sizes.append(img.header.get_zooms())\n        shapes.append(data.shape)\n\n    # pixdim based on variables in https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n    mean_spacing = np.mean(voxel_sizes, axis=0)\n    mean_shape = np.mean(shapes, axis=0)\n    pixdim = tuple(round(s, 2) for s in mean_spacing)\n\n    # default for soft tissue\n    a_min, a_max = -200, 250\n\n    # detect GPU & RAM memory\n    try:\n        import GPUtil\n        gpus = GPUtil.getGPUs()\n        mem_free_gpu = max([gpu.memoryFree for gpu in gpus])  # in MB\n    except Exception:\n        mem_free_gpu = 0  # fallback to CPU\n\n    mem_free_ram = psutil.virtual_memory().available // (1024 * 1024)\n\n    # adjust preprocessing resolution based on memory\n    # values are randomized based on https://docs.monai.io/en/stable/transforms.html\n    if mem_free_gpu >= 20000:\n        spatial_size = [256, 256, 256]\n        batch_size = 2\n    elif mem_free_gpu >= 10000:\n        spatial_size = [192, 192, 128]\n        batch_size = 1\n    elif mem_free_gpu >= 4000:\n        spatial_size = [128, 128, 64]\n        batch_size = 1\n    else:\n        spatial_size = [96, 96, 64]\n        batch_size = 1\n\n    return {\n        \"train_files\": train_files,\n        \"validation_files\": validation_files,\n        \"pixdim\": pixdim,\n        \"a_min\": a_min,\n        \"a_max\": a_max,\n        \"spatial_size\": spatial_size,\n        \"batch_size\": batch_size,\n        \"mem_free_gpu\": mem_free_gpu,\n        \"mem_free_ram\": mem_free_ram,\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T04:37:44.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom glob import glob\nfrom monai.transforms import (\n    Compose,\n    EnsureChannelFirstD,\n    LoadImaged,\n    Resized,\n    ToTensord,\n    Spacingd,\n    Orientationd,\n    ScaleIntensityRanged,\n    CropForegroundd,\n    RandCropByPosNegLabeld,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism\n\n\"\"\"\n    Use MONAI transforms to prepares data for segmentation.\n    Voxel: 3D grid representation of data.\n    \n    :param tuple pixdim: standard voxel spacing (in millimeters) for resampling the images in the x, y, and z dimensions.\n    :param int a_min: intensity voxel min for CT scans (less are clipped before scaling).\n    :param int a_max: intensity voxel max for CT scans (more are clipped before scaling).\n    :param int array spatial_size: output size (in voxel) to which each image and label volume will be resized. AKA input size for the neural network.\n    :param int batch_size: adjyst batch size, default is 1.\n    :return PyTorch DataLoader objects: used to train neural network.\n\"\"\"\ndef preprocess(pixdim, a_min, a_max, spatial_size, batch_size, cache, train_files, validation_files):\n\n    # reproduce training results\n    set_determinism(seed=0)\n\n    # and apply transformations to them\n    # parameters from https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n    train_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        RandCropByPosNegLabeld(\n            keys=[\"vol\", \"seg\"],\n            label_key=\"seg\",\n            spatial_size=spatial_size,  # use your configured size here\n            pos=1, neg=1,\n            num_samples=4,\n            image_key=\"vol\",\n            image_threshold=0,\n        ),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n\n    # transforms for validation data\n    validation_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n    ])\n\n    if cache >= 16000:\n        train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0)\n        val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0)\n\n        # train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n        # val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n    else:\n        train_ds = Dataset(data=train_files, transform=train_transforms)\n        validation_ds = Dataset(data=validation_files, transform=validation_transforms)\n\n        # train_ds = Dataset(data=train_files, transform=train_transforms, num_workers=4)\n        # validation_ds = Dataset(data=validation_files, transform=validation_transforms, num_workers=4)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size)\n    validation_loader = DataLoader(validation_ds, batch_size=batch_size)\n\n    # use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    # train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n    # validation_loader = DataLoader(validation_ds, batch_size=batch_size, num_workers=4)\n\n    return train_loader, validation_loader","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T04:37:44.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.transforms import (\n    AsDiscrete,\n    Compose,\n)\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef train (model, loss_function, optimizer, dice_metric):\n    max_epochs = 600\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = []\n    metric_values = []\n    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n    post_label = Compose([AsDiscrete(to_onehot=2)])\n    scaler = GradScaler()\n    \n    for epoch in range(max_epochs):\n        print(\"-\" * 10)\n        print(f\"epoch {epoch + 1}/{max_epochs}\")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = (\n                batch_data[\"image\"].to(device),\n                batch_data[\"label\"].to(device),\n            )\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(inputs)\n                loss = loss_function(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n                \n            # loss.backward()\n            # optimizer.step()\n            print(f\"loss.item: {loss.item()}\")\n            epoch_loss += loss.item()\n            print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n    \n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                for val_data in val_loader:\n                    val_inputs, val_labels = (\n                        val_data[\"image\"].to(device),\n                        val_data[\"label\"].to(device),\n                    )\n                    # roi_size = (160, 160, 160)\n                    roi_size = (64, 64, 64)\n                    sw_batch_size = 4\n                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n                    # compute metric for current iteration\n                    dice_metric(y_pred=val_outputs, y=val_labels)\n\n                    # free memory\n                    del val_inputs, val_labels, val_outputs\n                    torch.cuda.empty_cache()\n    \n                # aggregate the final mean dice result\n                metric = dice_metric.aggregate().item()\n                # reset the status for next validation round\n                dice_metric.reset()\n    \n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n                    print(\"saved new best metric model\")\n                print(\n                    f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n                    f\"\\nbest mean dice: {best_metric:.4f} \"\n                    f\"at epoch: {best_metric_epoch}\"\n                )\n    print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T04:37:44.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\nfrom monai.metrics import DiceMetric\n\nif __name__ == '__main__':\n    \n    # 1. user input (for now, it is kaggle data set)\n    params = prepare_and_configure(in_dir=\"/kaggle/input\")\n    \n    # show output\n    # print(\"prepare_and_configure:\")\n    # for k, v in params.items():\n    #     print(f\"{k}: {v}\")\n    \n    # 2. preprocess\n    train_loader, validation_loader = preprocess(pixdim=params['pixdim'], a_min=params['a_min'], a_max=params['a_max'], spatial_size=params['spatial_size'], batch_size=params['batch_size'], cache=params['mem_free_ram'], train_files=params['train_files'], validation_files=params['validation_files'])\n    # print(train_loader)\n    # print(validation_loader)\n\n    # 3. build U-net\n    device = torch.device(\"cuda:0\")\n    model = UNet(\n        spatial_dims=3,\n        in_channels=1,\n        out_channels=2,\n        channels=(8, 16, 32, 64),\n        strides=(2, 2, 2, 2),\n        num_res_units=1,\n        norm=Norm.BATCH,\n    ).to(device)\n    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n\n    # 4. train\n    train(model, loss_function, optimizer, dice_metric)\n\n    # 5. test","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T04:37:44.238Z"}},"outputs":[],"execution_count":null}]}