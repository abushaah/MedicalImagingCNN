{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n\"\"\"\n    Get data inputs, assumes CT volumes and segmentation masks have corresponding names and indices.\n    :param str in_dir: file path of data.\n\"\"\"\ndef prepare(in_dir=\"/kaggle/input\"):\n\n    volume_dict = {}\n    segmentation_dict = {}\n\n    for dirname, _, filenames in os.walk(in_dir):\n        for filename in filenames:\n            if filename.endswith(\".nii\"):\n                filepath = os.path.join(dirname, filename)\n                if filename.startswith(\"volume-\"):\n                    idx = int(filename.split(\"-\")[1].split(\".\")[0])\n                    volume_dict[idx] = filepath\n                elif filename.startswith(\"segmentation-\"):\n                    idx = int(filename.split(\"-\")[1].split(\".\")[0])\n                    segmentation_dict[idx] = filepath\n\n    # match volume and segmentation by idx\n    matched_keys = sorted(set(volume_dict.keys()) & set(segmentation_dict.keys()))\n    all_files = [{\"vol\": volume_dict[k], \"seg\": segmentation_dict[k]} for k in matched_keys]\n\n    # split into 80 train and 20 test\n    split_idx = int(0.8 * len(all_files))\n    train_files = all_files[:split_idx]\n    test_files = all_files[split_idx:]\n\n    return train_files, test_files","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nWritten by abushaah with sources:\nhttps://github.com/Project-MONAI/MONAI\nhttps://github.com/amine0110/Liver-Segmentation-Using-Monai-and-PyTorch/blob/main/preporcess.py\n'''\n\nimport re\nfrom glob import glob\nfrom monai.transforms import (\n    Compose,\n    EnsureChannelFirstD,\n    LoadImaged,\n    Resized,\n    ToTensord,\n    Spacingd,\n    Orientationd,\n    ScaleIntensityRanged,\n    CropForegroundd,\n)\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism\n\n\"\"\"\n    Use MONAI transforms to prepares data for segmentation.\n    Voxel: 3D grid representation of data.\n    \n    :param tuple pixdim: standard voxel spacing (in millimeters) for resampling the images in the x, y, and z dimensions.\n    :param int a_min: intensity voxel min for CT scans (less are clipped before scaling).\n    :param int a_max: intensity voxel max for CT scans (more are clipped before scaling).\n    :param int array spatial_size: output size (in voxel) to which each image and label volume will be resized. AKA input size for the neural network.\n    :param int batch_size: adjyst batch size, default is 1.\n    :return PyTorch DataLoader objects: used to train neural network.\n\"\"\"\ndef preprocess(pixdim=(1.5, 1.5, 1.0), a_min=-200, a_max=200, spatial_size=[128, 128, 64], batch_size=1):\n    train_files, test_files = prepare()\n\n    # reproduce training results\n    set_determinism(seed=0)\n\n    # and apply transformations to them\n    train_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n\n    # transforms for test data\n    test_transforms = Compose([\n        LoadImaged(keys=[\"vol\", \"seg\"]),\n        EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n        Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n        ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n        CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n        Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n        ToTensord(keys=[\"vol\", \"seg\"]),\n    ])\n    \n    train_ds = Dataset(data=train_files, transform=train_transforms)\n    test_ds = Dataset(data=test_files, transform=test_transforms)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size)\n    test_loader = DataLoader(test_ds, batch_size=batch_size)\n\n    return train_loader, test_loader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}